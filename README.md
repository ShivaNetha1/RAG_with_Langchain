# ğŸ§  Retrieval Augmented Generation (RAG) with LangChain & IBM Granite

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline using [LangChain](https://www.langchain.com/), IBM Granite models, and Milvus as the vector database. The RAG architecture enhances language models by providing relevant retrieved context from a knowledge base before generating a response.

---

## ğŸ“Œ Use Cases

- ğŸ§¾ **Customer Support**: Answer FAQs from product manuals
- ğŸ’¼ **Domain-Specific QA**: Legal, financial, or scientific document exploration
- ğŸ“° **News Chat**: Chat about current events using updated articles

---

## ğŸ§° Tech Stack

- ğŸ§  LLM: `ibm-granite` models (via Replicate)
- ğŸ“š Embeddings: `granite-embedding-30m-english`
- ğŸ—‚ï¸ Vector DB: `Milvus`
- ğŸ§± Framework: `LangChain`
- ğŸ“¦ Environment: Python 3.10+ (preferably in a virtual environment)

---

## âš™ï¸ Setup Instructions

### 1. âœ… Prerequisites

- Python 3.10, 3.11, or 3.12
- [Replicate](https://replicate.com/) API key
- Git, pip

### 2. ğŸ“¦ Install Dependencies

```bash
pip install git+https://github.com/ibm-granite-community/utils \
    transformers \
    langchain_community \
    'langchain_huggingface[full]' \
    langchain_milvus \
    replicate \
    wget
```

---

## ğŸš€ Running the Notebook

1. Clone this repo or open `RAG_with_Langchain.ipynb` in Colab/Jupyter.
2. Ensure your Python version is correct:
   ```python
   import sys
   assert sys.version_info >= (3, 10) and sys.version_info < (3, 13)
   ```
3. Install required packages (see above).
4. Set your Replicate API key:
   ```bash
   export REPLICATE_API_TOKEN=your_token_here
   ```
5. Choose your models:
   - **Embeddings**: `ibm-granite/granite-embedding-30m-english`
   - **LLM**: `ibm-granite/granite-3.3-8b-instruct`
6. Use `state_of_the_union.txt` or any other document for indexing of your own.
7. Run the entire notebook to:
   - Load & chunk documents
   - Embed chunks and populate Milvus
   - Build a LangChain RAG pipeline
   - Query with semantic retrieval and get LLM responses

---

## ğŸ§ª Example Query

```python
query = "What did the president say about Ketanji Brown Jackson?"
output = rag_chain.invoke({"input": query})
print(output['answer'])
```

**Output:**
> The president nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the Supreme Court, praising her legal expertise and bipartisan support.

---

## ğŸ—‚ï¸ File Structure

```
.
â”œâ”€â”€ RAG_with_Langchain.ipynb   # Main notebook
â”œâ”€â”€ state_of_the_union.txt     # Example document
â””â”€â”€ README.md                  # Project instructions
```

---

## âœ… Tips

- Replace Milvus with any other LangChain-supported vector store (e.g., FAISS, Chroma).
- Replace Replicate + Granite with OpenAI, Cohere, etc., by swapping out the LLM component.
- Add document upload support to dynamically build knowledge bases.

---

## ğŸ“„ License

This project is licensed under the MIT License.
